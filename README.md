<h1 align="center">âš¡ UK-DALE NILM â€” Linear / Logistic / MLP Baselines</h1>
<p align="center">
  <b>Building-1, 1-min aggregation, 14-day window</b><br/>
  Linear Regression Â· Logistic Regression Â· MLP Â· Multi-Label Learning
</p>

---

## ğŸ“Œ Overview
This project builds strong **Non-Intrusive Load Monitoring (NILM)** baselines on the UK-DALE dataset using:
- **Linear Regression** â€” Watt-level estimation  
- **Logistic Regression** â€” Binary ON/OFF classification  
- **Multi-Layer Perceptron (MLP)** â€” Context-aware deep learning  
- **Multi-label joint models** â€” Kettle, Microwave, Fridge-Freezer, Toaster  

Each model was trained on **Building 1**, aggregated at **1-minute resolution**, using **14-day windows** (Marchâ€“April 2013).

---

## ğŸ“Š Model Performance Summary

<details>
<summary><b>Single-Appliance (Kettle)</b></summary>

| Model | AUC | Precision | Recall | F1 | Note |
|---|---:|---:|---:|---:|---|
| Linear Regression | â€” | â€” | â€” | **MAE â‰ˆ 76 W** | Power-level baseline |
| Logistic (tuned) | **0.977** | 0.46 | 0.87 | 0.61 | Threshold tuned for recall |
| MLP (14d tuned) | **0.999** | **0.83** | **0.85** | **0.84** | âœ… Deployed baseline |
</details>

<details>
<summary><b>Multi-Appliance (14d MLP)</b></summary>

| Appliance | AUC | Precision | Recall | F1 | Comment |
|---|---:|---:|---:|---:|---|
| Kettle | 0.995â€“0.998 | 0.47â€“0.83 | 0.85â€“0.98 | 0.61â€“0.84 | Sharp spikes |
| Microwave | 0.986â€“0.996 | 0.71â€“0.97 | 0.29â€“0.39 | 0.44â€“0.50 | Sparse events |
| Fridge-Freezer | 0.922â€“0.954 | **0.85â€“0.87** | **0.63â€“0.75** | **0.72â€“0.81** | Quasi-always-on |
| Toaster | 0.990â€“0.938 | 0.34â€“1.00 | 0.67â€“0.95 | 0.45â€“0.65 | Tiny bursts |
</details>

---

## ğŸ“ˆ Visual Gallery

### ğŸ”¹ Alignment (Ground Truth vs Mains)
<p align="center">
  <img src="plots/preview.png" width="750" alt="Mains vs Kettle preview"/>
</p>

### ğŸ”¹ Regression & Logistic Models
<p align="center">
  <img src="plots/linreg_kettle_test_overlay.png" width="240" />
  <img src="plots/logreg_kettle_threshold_tuning.png" width="240" />
  <img src="plots/logreg_kettle_rolling_tuning.png" width="240" />
</p>

### ğŸ”¹ Neural Networks (MLP)
<table>
  <tr>
    <td align="center">
      <img src="plots/mlp_kettle_probs.png" width="320" />
      <br/><sub>MLP probability trajectory (7â€“14d context).</sub>
    </td>
    <td align="center">
      <img src="plots/mlp_kettle_14d_prob.png" width="320" />
      <br/><sub>14-day context boosts recall and smooths predictions.</sub>
    </td>
    <td align="center">
      <img src="plots/mlp_kettle_14d_decisions.png" width="320" />
      <br/><sub>Tuned decision overlay (thresholded ON/OFF).</sub>
    </td>
  </tr>
</table>

### ğŸ”¹ Multi-Label Overview
<p align="center">
  <img src="plots/multilabel_confusion.png" width="780" />
  <br/><sub>Aggregated confusion across Kettle, Microwave, Fridge-Freezer, Toaster.</sub>
</p>

---

## ğŸ§  Key Insights

- **Temporal context (14d > 7d)** significantly boosts recall for short-duration spikes.  
- **Threshold tuning (â‰¥0.85 recall)** balances precisionâ€“recall trade-offs.  
- **Fridge-Freezer & Toaster** gain the most from nonlinear MLP transformations.  
- **Multi-label joint setup** enhances appliance robustness and generalization.  
- **Rolling logistic regression** improves prediction stability for noisy loads.

---

## ğŸš€ Next Steps
- ğŸ§© Handle class imbalance via weighted BCE or SMOTE.  
- ğŸ§® Integrate **Bayesian optimization** for hyperparameter tuning.  
- âš™ï¸ Extend to **sequence models (LSTM, Transformer)** for temporal reasoning.  
- ğŸ  Test **cross-building transfer** (B1 â†’ B2â€“B5).  
- ğŸ§­ Add a lightweight **Streamlit dashboard** for visual NILM comparisons.

---

## ğŸ§¾ License
This repository is licensed under the **MIT License** â€” see the [LICENSE](./LICENSE) file for details.

---

## ğŸ“š Citation
If you use this work, please cite:

